fig = ggplot(melt(input_data, id=c('Species', 'X.axis')), aes(x=X.axis, y=as.numeric(value), fill=variable)) +
geom_bar(position='dodge2', stat='identity') +
labs(x = "", fill = "") +
xlab('Minor allele frequency') +
ylab('Proportion of segregating sites') +
theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) +
scale_fill_manual(values=c("blue4", "steelblue3", "goldenrod3", "goldenrod1")) +
# scale_fill_manual(values=c("#cb181d", "#fb6a4a", "blue4", "steelblue3"), name='Site-frequency-spectra') +
theme(legend.position="none") +
theme(plot.title = element_text(face = "italic", size=16)) +
theme(axis.text=element_text(size=12),
axis.title=element_text(size=16))
return(fig)
}
plot_dfe_grid = function(input) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('shape', 'scale', 'likelihood')
unique_shape = unique(species_surface$shape)
unique_scale = unique(species_surface$scale)
Z = matrix(data=NA, nrow=length(unique_shape), ncol=length(unique_scale))
count = 1
for (i in 1:length(unique_shape)) {
for (j in 1:length(unique_scale)) {
Z[i, j] = species_surface$likelihood[count]
if (species_surface$shape[count] != unique_shape[i]) {
print('break')
} else if (species_surface$scale[count] != unique_scale[j]) {
print('break')
}
count = count + 1
}
}
species_surface = species_surface[order(species_surface$likelihood, decreasing=TRUE), ]
best_params = c(species_surface$shape[1], species_surface$scale[1])
print(best_params)
MLE = max(species_surface$likelihood)
species_surface$likelihood = species_surface$likelihood - MLE
color_breakpoints = cut(species_surface$likelihood, c(-Inf, -3, -1, -0.5, 0))
likelihood_surface_title = paste('MLE @ [', str_trunc(toString(best_params[1]), 8, ellipsis=''), sep='')
likelihood_surface_title = paste(likelihood_surface_title, ', ', sep='')
likelihood_surface_title = paste(likelihood_surface_title, str_trunc(toString(best_params[2]), 8, ellipsis=''), sep='')
likelihood_surface_title = paste(likelihood_surface_title, ']', sep='')
fig = ggplot(species_surface) +
geom_contour_filled(aes(x=shape, y=scale, z=likelihood),
breaks = c(0, -0.5, -1, -3, -Inf)) +
scale_fill_brewer(palette = "YlGnBu", direction=1, name='Log Likelihood') +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) +
annotate('point', x=best_params[1], y=best_params[2], color='orange', size=2) +
xlab('Shape')  +
ylab('Scale') +
scale_y_log10()
#ggtitle(likelihood_surface_title)
return(fig)
}
find_dfe_mle = function(input) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('shape', 'scale', 'likelihood')
unique_shape = unique(species_surface$shape)
unique_scale = unique(species_surface$scale)
Z = matrix(data=NA, nrow=length(unique_shape), ncol=length(unique_scale))
count = 1
species_surface = species_surface[order(species_surface$likelihood, decreasing=TRUE), ]
best_params = c(species_surface$shape[1], species_surface$scale[1])
print(best_params)
MLE = max(species_surface$likelihood)
print(MLE)
}
cross_species_dfe_comparison = function(input_A, input_B) {
species_surface_A = read.csv(input_A, header=TRUE)
names(species_surface_A) = c('shape', 'scale', 'likelihood')
unique_shape_A = unique(species_surface_A$shape)
unique_scale_A = unique(species_surface_A$scale)
Z_A = matrix(data=NA, nrow=length(unique_shape_A), ncol=length(unique_scale_A))
count = 1
for (i in 1:length(unique_shape_A)) {
for (j in 1:length(unique_scale_A)) {
Z_A[i, j] = species_surface_A$likelihood[count]
if (species_surface_A$shape[count] != unique_shape_A[i]) {
print('break')
} else if (species_surface_A$scale[count] != unique_scale_A[j]) {
print('break')
}
count = count + 1
}
}
temp_surface_A = species_surface_A[order(species_surface_A$likelihood, decreasing=TRUE), ]
best_params_A = c(temp_surface_A$shape[1], temp_surface_A$scale[1])
ML_A = temp_surface_A$likelihood[1]
species_surface_B = read.csv(input_B, header=TRUE)
names(species_surface_B) = c('shape', 'scale', 'likelihood')
unique_shape_B = unique(species_surface_B$shape)
unique_scale_B = unique(species_surface_B$scale)
Z_B = matrix(data=NA, nrow=length(unique_shape_B), ncol=length(unique_scale_B))
count = 1
for (i in 1:length(unique_shape_B)) {
for (j in 1:length(unique_scale_B)) {
Z_B[i, j] = species_surface_B$likelihood[count]
if (species_surface_B$shape[count] != unique_shape_B[i]) {
print('break')
} else if (species_surface_B$scale[count] != unique_scale_B[j]) {
print('break')
}
count = count + 1
}
}
temp_surface_B = species_surface_B[order(species_surface_B$likelihood, decreasing=TRUE), ]
best_params_B = c(temp_surface_B$shape[1], temp_surface_B$scale[1])
ML_B = temp_surface_B$likelihood[1]
combined_likelihood = species_surface_A$likelihood + species_surface_B$likelihood
comparison_surface = data.frame(species_surface_A$shape, species_surface_A$scale, combined_likelihood)
temp_comparison_surface = comparison_surface[order(comparison_surface$combined_likelihood, decreasing=TRUE), ]
best_params_comparison = c(temp_comparison_surface$species_surface_A.shape[1], temp_comparison_surface$species_surface_A.scale[1])
ML_comparison = temp_comparison_surface$combined_likelihood[1]
independent_sum = ML_A + ML_B
return(ML_comparison - independent_sum)
}
compare_core_accessory_sfs = function(all, core, accessory) {
x_axis = 1:length(all)
input_df = data.frame(proportional_sfs(all),
proportional_sfs(core),
proportional_sfs(accessory),
x_axis)
names(input_df) = c('All genes',
'Core genes',
'Accessory genes',
'x_axis')
p_input_comparison <- ggplot(data = melt(input_df, id='x_axis'),
aes(x=x_axis,
y=value,
fill=variable)) +
geom_bar(position='dodge2', stat='identity') +
labs(x = "", fill = "") +
scale_x_continuous(name='Minor allele frequency in Sample', breaks=x_axis, limits=c(0.5, length(x_axis) + 0.5)) +
ylab('Proportion of segregating sites') +
theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
## scale_fill_manual(values=c("darkslateblue", "darkslategrey", "darkturquoise"))
return(p_input_comparison)
}
compare_core_accessory_sfs_count = function(all, core, accessory) {
x_axis = 1:length(all)
input_df = data.frame(all,
core,
accessory,
x_axis)
names(input_df) = c('All genes',
'Core genes',
'Accessory genes',
'x_axis')
p_input_comparison <- ggplot(data = melt(input_df, id='x_axis'),
aes(x=x_axis,
y=value,
fill=variable)) +
geom_bar(position='dodge2', stat='identity') +
labs(x = "", fill = "") +
scale_x_continuous(name='Minor allele frequency in Sample', breaks=x_axis, limits=c(0.5, length(x_axis) + 0.5)) +
ylab('Proportion of segregating sites') +
theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
## scale_fill_manual(values=c("darkslateblue", "darkslategrey", "darkturquoise"))
return(p_input_comparison)
}
compare_core_accessory_sfs_syn_ns = function(core_syn, core_nonsyn, accessory_syn, accessory_nonsyn) {
x_axis = 1:length(core_syn)
input_df = data.frame(proportional_sfs(core_syn),
proportional_sfs(core_nonsyn),
proportional_sfs(accessory_syn),
proportional_sfs(accessory_nonsyn),
x_axis)
names(input_df) = c('Core genes (Syn)',
'Core genes (Nonsyn)',
'Accessory genes (Syn)',
'Accessory genes (Nonsyn)',
'x_axis')
p_input_comparison <- ggplot(data = melt(input_df, id='x_axis'),
aes(x=x_axis,
y=value,
fill=variable)) +
geom_bar(position='dodge2', stat='identity') +
labs(x = "", fill = "") +
scale_x_continuous(name='Minor allele frequency in Sample', breaks=x_axis, limits=c(0.5, length(x_axis) + 0.5)) +
ylab('Proportion of segregating sites') +
theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))+
scale_fill_manual(values=c("blue4", "steelblue3", "goldenrod3", "goldenrod1"))
return(p_input_comparison)
}
compare_core_sfs = function(all, core) {
x_axis = 1:length(all)
input_df = data.frame(proportional_sfs(all),
proportional_sfs(core),
x_axis)
names(input_df) = c('All genes',
'Core genes',
'x_axis')
p_input_comparison <- ggplot(data = melt(input_df, id='x_axis'),
aes(x=x_axis,
y=value,
fill=variable)) +
geom_bar(position='dodge2', stat='identity') +
labs(x = "", fill = "") +
scale_x_continuous(name='Minor allele frequency in Sample', breaks=x_axis, limits=c(0.5, length(x_axis) + 0.5)) +
ylab('Proportion of segregating sites') +
theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),
panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"))
## scale_fill_manual(values=c("darkslateblue", "darkslategrey", "darkturquoise"))
return(p_input_comparison)
}
extract_array_length <- function(input_string) {
array_string <- str_extract(input_string, "\\[(.*?)\\]")
num_elements <- length(strsplit(array_string, "[ ,]")[[1]])
return(num_elements)
}
AIC_from_demography = function(input_file) {
## Reads input SFS from output *demography.txt
if(grepl("one_epoch", input_file)) {
k=2
} else if(grepl("two_epoch", input_file)) {
k=4
} else {
k=8
}
this_file = file(input_file)
on.exit(close(this_file))
ll_string = readLines(this_file)[2]
loglik <- as.numeric(str_extract(ll_string, "-?\\d+\\.\\d+"))
return(k - 2*loglik)
}
return_nu_high = function(input) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('index', 'nu', 'tau', 'likelihood')
MLE = max(species_surface$likelihood)
# Task 1: Remove rows with likelihood less than MLE - 3
species_surface <- species_surface[species_surface$likelihood >= MLE - 3, ]
# Task 2: Get the highest nu value from the remaining rows
highest_nu <- max(species_surface$nu)
return(highest_nu)
}
return_nu_low = function(input) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('index', 'nu', 'tau', 'likelihood')
MLE = max(species_surface$likelihood)
# Task 1: Remove rows with likelihood less than MLE - 3
species_surface <- species_surface[species_surface$likelihood >= MLE - 3, ]
# Task 2: Get the highest nu value from the remaining rows
lowest_nu <- min(species_surface$nu)
return(lowest_nu)
}
return_nu_mle = function(input) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('index', 'nu', 'tau', 'likelihood')
species_surface = species_surface[order(species_surface$likelihood, decreasing=TRUE), ]
return(species_surface$nu[1])
}
return_time_high = function(input, sfs_file, theta_file) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('index', 'nu', 'tau', 'likelihood')
MLE = max(species_surface$likelihood)
# Task 1: Remove rows with likelihood less than MLE - 3
species_surface <- species_surface[species_surface$likelihood >= MLE - 3, ]
# Task 2: Get the highest nu value from the remaining rows
highest_tau <- max(species_surface$tau)
# Read the contents of the file into a variable
sfs_lines <- readLines(sfs_file)
# Extract the second line and split it into individual values
sfs_line <- sfs_lines[2]
sfs_vector <- as.numeric(unlist(strsplit(sfs_line, " ")))
allele_sum = sum(sfs_vector)
# Read the contents of the file into a variable
theta_lines <- readLines(theta_file)
# Extract the fifth line
theta_line <- theta_lines[5]
theta <- as.numeric(regmatches(theta_line, regexpr("\\d+\\.\\d+", theta_line)))
mu_low = 4.08E-10
generations_high = 2 * highest_tau * theta / (4 * mu_low * allele_sum)
years = 2 * highest_tau * theta / (4 * 4.08E-10 * allele_sum * 365)
return(years)
}
return_time_low = function(input, sfs_file, theta_file) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('index', 'nu', 'tau', 'likelihood')
MLE = max(species_surface$likelihood)
# Task 1: Remove rows with likelihood less than MLE - 3
species_surface <- species_surface[species_surface$likelihood >= MLE - 3, ]
# Task 2: Get the highest nu value from the remaining rows
lowest_tau <- min(species_surface$tau)
# Read the contents of the file into a variable
sfs_lines <- readLines(sfs_file)
# Extract the second line and split it into individual values
sfs_line <- sfs_lines[2]
sfs_vector <- as.numeric(unlist(strsplit(sfs_line, " ")))
allele_sum = sum(sfs_vector)
# Read the contents of the file into a variable
theta_lines <- readLines(theta_file)
# Extract the fifth line
theta_line <- theta_lines[5]
theta <- as.numeric(regmatches(theta_line, regexpr("\\d+\\.\\d+", theta_line)))
mu_low = 4.08E-10
generations_high = 2 * lowest_tau * theta / (4 * mu_low * allele_sum)
years = 2 * lowest_tau * theta / (4 * 4.08E-10 * allele_sum * 365)
return(years)
}
return_time_mle = function(input, sfs_file, theta_file) {
species_surface = read.csv(input, header=TRUE)
names(species_surface) = c('index', 'nu', 'tau', 'likelihood')
species_surface = species_surface[order(species_surface$likelihood, decreasing=TRUE), ]
mle_tau = species_surface$tau[i]
# Read the contents of the file into a variable
sfs_lines <- readLines(sfs_file)
# Extract the second line and split it into individual values
sfs_line <- sfs_lines[2]
sfs_vector <- as.numeric(unlist(strsplit(sfs_line, " ")))
allele_sum = sum(sfs_vector)
# Read the contents of the file into a variable
theta_lines <- readLines(theta_file)
# Extract the fifth line
theta_line <- theta_lines[5]
theta <- as.numeric(regmatches(theta_line, regexpr("\\d+\\.\\d+", theta_line)))
mu_low = 4.08E-10
generations_high = 2 * mle_tau * theta / (4 * mu_low * allele_sum)
years = 2 * mle_tau * theta / (4 * 4.08E-10 * allele_sum * 365)
return(years)
}
read_demography_info <- function(filepath) {
# Read the content of the file
file_content <- readLines(filepath)
# Extract the relevant lines
nu <- as.numeric(regmatches(file_content[1], regexpr("\\d+\\.\\d+", file_content[1])))
low_years <- as.numeric(regmatches(file_content[length(file_content)-3], regexpr("\\d+\\.\\d+", file_content[length(file_content)-3])))
low_ancestral_size <- as.numeric(regmatches(file_content[length(file_content)-1], regexpr("\\d+\\.\\d+", file_content[length(file_content)-1])))
# Create a vector with the extracted information
result_vector <- c(nu, low_years, low_ancestral_size)
return(result_vector)
}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
pchisq(274.376417233, 1, lower.tail=FALSE)
update.packages()
## Tennessen simulation
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source('useful_functions.R')
global_allele_sum = 1000000
# 2EpC 10,20,30,50,100,150,200,300,500,700
TwoEpochC_empirical_file_list = list()
TwoEpochC_one_epoch_file_list = list()
TwoEpochC_two_epoch_file_list = list()
TwoEpochC_three_epoch_file_list = list()
TwoEpochC_one_epoch_AIC = c()
TwoEpochC_one_epoch_LL = c()
TwoEpochC_one_epoch_theta = c()
TwoEpochC_one_epoch_allele_sum = global_allele_sum
TwoEpochC_two_epoch_AIC = c()
TwoEpochC_two_epoch_LL = c()
TwoEpochC_two_epoch_theta = c()
TwoEpochC_two_epoch_nu = c()
TwoEpochC_two_epoch_tau = c()
TwoEpochC_two_epoch_allele_sum = global_allele_sum
TwoEpochC_three_epoch_AIC = c()
TwoEpochC_three_epoch_LL = c()
TwoEpochC_three_epoch_theta = c()
TwoEpochC_three_epoch_nuB = c()
TwoEpochC_three_epoch_nuF = c()
TwoEpochC_three_epoch_tauB = c()
TwoEpochC_three_epoch_tauF = c()
TwoEpochC_three_epoch_allele_sum = global_allele_sum
# Loop through subdirectories and get relevant files
for (i in c(10, 20, 30, 50, 100, 150, 200, 300, 500, 700)) {
subdirectory <- paste0("../Analysis/TwoEpochContraction_", i)
TwoEpochC_empirical_file_path <- file.path(subdirectory, "syn_downsampled_sfs.txt")
TwoEpochC_one_epoch_file_path <- file.path(subdirectory, "one_epoch_demography.txt")
TwoEpochC_two_epoch_file_path <- file.path(subdirectory, "two_epoch_demography.txt")
TwoEpochC_three_epoch_file_path <- file.path(subdirectory, "three_epoch_demography.txt")
TwoEpochC_true_demography_file_path <- file.path(subdirectory, 'true_demography.txt')
# Check if the file exists before attempting to read and print its contents
if (file.exists(TwoEpochC_empirical_file_path)) {
this_empirical_sfs = read_input_sfs(TwoEpochC_empirical_file_path)
TwoEpochC_empirical_file_list[[subdirectory]] = this_empirical_sfs
}
if (file.exists(TwoEpochC_one_epoch_file_path)) {
this_one_epoch_sfs = sfs_from_demography(TwoEpochC_one_epoch_file_path)
TwoEpochC_one_epoch_file_list[[subdirectory]] = this_one_epoch_sfs
TwoEpochC_one_epoch_AIC = c(TwoEpochC_one_epoch_AIC, AIC_from_demography(TwoEpochC_one_epoch_file_path))
TwoEpochC_one_epoch_LL = c(TwoEpochC_one_epoch_LL, LL_from_demography(TwoEpochC_one_epoch_file_path))
TwoEpochC_one_epoch_theta = c(TwoEpochC_one_epoch_theta, theta_from_demography(TwoEpochC_one_epoch_file_path))
# TwoEpochC_one_epoch_allele_sum = c(TwoEpochC_one_epoch_allele_sum, sum(this_one_epoch_sfs))
}
if (file.exists(TwoEpochC_two_epoch_file_path)) {
this_two_epoch_sfs = sfs_from_demography(TwoEpochC_two_epoch_file_path)
TwoEpochC_two_epoch_file_list[[subdirectory]] = this_two_epoch_sfs
TwoEpochC_two_epoch_AIC = c(TwoEpochC_two_epoch_AIC, AIC_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_LL = c(TwoEpochC_two_epoch_LL, LL_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_theta = c(TwoEpochC_two_epoch_theta, theta_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_nu = c(TwoEpochC_two_epoch_nu, nu_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_tau = c(TwoEpochC_two_epoch_tau, tau_from_demography(TwoEpochC_two_epoch_file_path))
# TwoEpochC_two_epoch_allele_sum = c(TwoEpochC_two_epoch_allele_sum, sum(this_two_epoch_sfs))
}
if (file.exists(TwoEpochC_three_epoch_file_path)) {
this_three_epoch_sfs = sfs_from_demography(TwoEpochC_three_epoch_file_path)
TwoEpochC_three_epoch_file_list[[subdirectory]] = this_three_epoch_sfs
TwoEpochC_three_epoch_AIC = c(TwoEpochC_three_epoch_AIC, AIC_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_LL = c(TwoEpochC_three_epoch_LL, LL_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_theta = c(TwoEpochC_three_epoch_theta, theta_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_nuB = c(TwoEpochC_three_epoch_nuB, nuB_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_nuF = c(TwoEpochC_three_epoch_nuF, nuF_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_tauB = c(TwoEpochC_three_epoch_tauB, tauB_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_tauF = c(TwoEpochC_three_epoch_tauF, tauF_from_demography(TwoEpochC_three_epoch_file_path))
# TwoEpochC_three_epoch_allele_sum = c(TwoEpochC_three_epoch_allele_sum, sum(this_three_epoch_sfs))
}
if (file.exists(TwoEpochC_true_demography_file_path)) {
this_true_demography_sfs = sfs_from_demography(TwoEpochC_true_demography_file_path)
TwoEpochC_true_demography_file_list[[subdirectory]] = this_true_demography_sfs
TwoEpochC_true_demography_AIC = c(TwoEpochC_true_demography_AIC, AIC_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_LL = c(TwoEpochC_true_demography_LL, LL_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_theta = c(TwoEpochC_true_demography_theta, theta_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_nu = c(TwoEpochC_true_demography_nu, nu_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_tau = c(TwoEpochC_true_demography_tau, tau_from_demography(TwoEpochC_true_demography_file_path))
}
}
TwoEpochC_true_demography_file_path
## Tennessen simulation
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
source('useful_functions.R')
global_allele_sum = 1000000
# 2EpC 10,20,30,50,100,150,200,300,500,700
TwoEpochC_empirical_file_list = list()
TwoEpochC_one_epoch_file_list = list()
TwoEpochC_two_epoch_file_list = list()
TwoEpochC_three_epoch_file_list = list()
TwoEpochC_true_demography_file_list = list()
TwoEpochC_one_epoch_AIC = c()
TwoEpochC_one_epoch_LL = c()
TwoEpochC_one_epoch_theta = c()
TwoEpochC_one_epoch_allele_sum = global_allele_sum
TwoEpochC_two_epoch_AIC = c()
TwoEpochC_two_epoch_LL = c()
TwoEpochC_two_epoch_theta = c()
TwoEpochC_two_epoch_nu = c()
TwoEpochC_two_epoch_tau = c()
TwoEpochC_two_epoch_allele_sum = global_allele_sum
TwoEpochC_three_epoch_AIC = c()
TwoEpochC_three_epoch_LL = c()
TwoEpochC_three_epoch_theta = c()
TwoEpochC_three_epoch_nuB = c()
TwoEpochC_three_epoch_nuF = c()
TwoEpochC_three_epoch_tauB = c()
TwoEpochC_three_epoch_tauF = c()
TwoEpochC_three_epoch_allele_sum = global_allele_sum
TwoEpochC_true_demography_AIC = c()
TwoEpochC_true_demography_LL = c()
TwoEpochC_true_demography_theta = c()
TwoEpochC_true_demography_allele_sum = global_allele_sum
TwoEpochC_true_demography_nu = c()
TwoEpochC_true_demography_tau = c()
# Loop through subdirectories and get relevant files
for (i in c(10, 20, 30, 50, 100, 150, 200, 300, 500, 700)) {
subdirectory <- paste0("../Analysis/TwoEpochContraction_", i)
TwoEpochC_empirical_file_path <- file.path(subdirectory, "syn_downsampled_sfs.txt")
TwoEpochC_one_epoch_file_path <- file.path(subdirectory, "one_epoch_demography.txt")
TwoEpochC_two_epoch_file_path <- file.path(subdirectory, "two_epoch_demography.txt")
TwoEpochC_three_epoch_file_path <- file.path(subdirectory, "three_epoch_demography.txt")
TwoEpochC_true_demography_file_path <- file.path(subdirectory, 'true_demography.txt')
# Check if the file exists before attempting to read and print its contents
if (file.exists(TwoEpochC_empirical_file_path)) {
this_empirical_sfs = read_input_sfs(TwoEpochC_empirical_file_path)
TwoEpochC_empirical_file_list[[subdirectory]] = this_empirical_sfs
}
if (file.exists(TwoEpochC_one_epoch_file_path)) {
this_one_epoch_sfs = sfs_from_demography(TwoEpochC_one_epoch_file_path)
TwoEpochC_one_epoch_file_list[[subdirectory]] = this_one_epoch_sfs
TwoEpochC_one_epoch_AIC = c(TwoEpochC_one_epoch_AIC, AIC_from_demography(TwoEpochC_one_epoch_file_path))
TwoEpochC_one_epoch_LL = c(TwoEpochC_one_epoch_LL, LL_from_demography(TwoEpochC_one_epoch_file_path))
TwoEpochC_one_epoch_theta = c(TwoEpochC_one_epoch_theta, theta_from_demography(TwoEpochC_one_epoch_file_path))
# TwoEpochC_one_epoch_allele_sum = c(TwoEpochC_one_epoch_allele_sum, sum(this_one_epoch_sfs))
}
if (file.exists(TwoEpochC_two_epoch_file_path)) {
this_two_epoch_sfs = sfs_from_demography(TwoEpochC_two_epoch_file_path)
TwoEpochC_two_epoch_file_list[[subdirectory]] = this_two_epoch_sfs
TwoEpochC_two_epoch_AIC = c(TwoEpochC_two_epoch_AIC, AIC_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_LL = c(TwoEpochC_two_epoch_LL, LL_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_theta = c(TwoEpochC_two_epoch_theta, theta_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_nu = c(TwoEpochC_two_epoch_nu, nu_from_demography(TwoEpochC_two_epoch_file_path))
TwoEpochC_two_epoch_tau = c(TwoEpochC_two_epoch_tau, tau_from_demography(TwoEpochC_two_epoch_file_path))
# TwoEpochC_two_epoch_allele_sum = c(TwoEpochC_two_epoch_allele_sum, sum(this_two_epoch_sfs))
}
if (file.exists(TwoEpochC_three_epoch_file_path)) {
this_three_epoch_sfs = sfs_from_demography(TwoEpochC_three_epoch_file_path)
TwoEpochC_three_epoch_file_list[[subdirectory]] = this_three_epoch_sfs
TwoEpochC_three_epoch_AIC = c(TwoEpochC_three_epoch_AIC, AIC_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_LL = c(TwoEpochC_three_epoch_LL, LL_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_theta = c(TwoEpochC_three_epoch_theta, theta_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_nuB = c(TwoEpochC_three_epoch_nuB, nuB_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_nuF = c(TwoEpochC_three_epoch_nuF, nuF_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_tauB = c(TwoEpochC_three_epoch_tauB, tauB_from_demography(TwoEpochC_three_epoch_file_path))
TwoEpochC_three_epoch_tauF = c(TwoEpochC_three_epoch_tauF, tauF_from_demography(TwoEpochC_three_epoch_file_path))
# TwoEpochC_three_epoch_allele_sum = c(TwoEpochC_three_epoch_allele_sum, sum(this_three_epoch_sfs))
}
if (file.exists(TwoEpochC_true_demography_file_path)) {
this_true_demography_sfs = sfs_from_demography(TwoEpochC_true_demography_file_path)
TwoEpochC_true_demography_file_list[[subdirectory]] = this_true_demography_sfs
TwoEpochC_true_demography_AIC = c(TwoEpochC_true_demography_AIC, AIC_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_LL = c(TwoEpochC_true_demography_LL, LL_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_theta = c(TwoEpochC_true_demography_theta, theta_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_nu = c(TwoEpochC_true_demography_nu, nu_from_demography(TwoEpochC_true_demography_file_path))
TwoEpochC_true_demography_tau = c(TwoEpochC_true_demography_tau, tau_from_demography(TwoEpochC_true_demography_file_path))
}
}
TwoEpochC_AIC_df = data.frame(TwoEpochC_one_epoch_AIC, TwoEpochC_two_epoch_AIC, TwoEpochC_three_epoch_AIC, TwoEpochC_true_demography_AIC)
# Reshape the data from wide to long format
TwoEpochC_df_long <- tidyr::gather(TwoEpochC_AIC_df, key = "Epoch", value = "AIC", TwoEpochC_one_epoch_AIC:TwoEpochC_three_epoch_AIC)
# Increase the x-axis index by 4
TwoEpochC_df_long$Index <- rep(c(10, 20, 30, 50, 100, 150, 200, 300, 500, 700), times = 4)
